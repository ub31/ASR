<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.69">
<title data-react-helmet="true">Experimental Result &amp; Analysis | Personalization of Generalized Activity Recognition Models</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_language" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Experimental Result &amp; Analysis | Personalization of Generalized Activity Recognition Models"><meta data-react-helmet="true" name="description" content="Experiment 1"><meta data-react-helmet="true" property="og:description" content="Experiment 1"><meta data-react-helmet="true" property="og:url" content="https://ub31.github.io/ASR/docs/doc8"><link data-react-helmet="true" rel="shortcut icon" href="/ASR/img/logohead.png"><link data-react-helmet="true" rel="canonical" href="https://ub31.github.io/ASR/docs/doc8"><link rel="stylesheet" href="/ASR/styles.edc99cb9.css">
<link rel="preload" href="/ASR/styles.71d10a56.js" as="script">
<link rel="preload" href="/ASR/runtime~main.366c8f14.js" as="script">
<link rel="preload" href="/ASR/main.6fc68446.js" as="script">
<link rel="preload" href="/ASR/1.8f67cab4.js" as="script">
<link rel="preload" href="/ASR/35.da5c47bd.js" as="script">
<link rel="preload" href="/ASR/36.a067ae9c.js" as="script">
<link rel="preload" href="/ASR/f976f453.247ede68.js" as="script">
<link rel="preload" href="/ASR/34.1489086d.js" as="script">
<link rel="preload" href="/ASR/b849be5b.bd3ae262.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav aria-label="Skip navigation links"><button type="button" tabindex="0" class="skipToContent_2AhQ">Skip to main content</button></nav><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/ASR/"><img src="/ASR/img/logo.jpeg" alt="My Site Logo" class="themedImage_2E_h themedImage--light_AouX navbar__logo"><img src="/ASR/img/logo.jpeg" alt="My Site Logo" class="themedImage_2E_h themedImage--dark_1YPN navbar__logo"><strong class="navbar__title">Home</strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ASR/docs/">Description</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/ASR/"><img src="/ASR/img/logo.jpeg" alt="My Site Logo" class="themedImage_2E_h themedImage--light_AouX navbar__logo"><img src="/ASR/img/logo.jpeg" alt="My Site Logo" class="themedImage_2E_h themedImage--dark_1YPN navbar__logo"><strong class="navbar__title">Home</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/ASR/docs/">Description</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive thin-scrollbar menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Project Description</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/ASR/docs/">Introduction</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/ASR/docs/doc3">Related Work &amp; Goals</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/ASR/docs/doc5">Dataset</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/ASR/docs/doc6">Exploratory analysis</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/ASR/docs/doc7">Methods</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/ASR/docs/doc8">Experimental Result &amp; Analysis</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/ASR/docs/doc9">Conclusion</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/ASR/docs/doc10">Future Work</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/ASR/docs/doc11">Appendix</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_39qw"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><header><h1 class="docTitle_1Lrw">Experimental Result &amp; Analysis</h1></header><div class="markdown"><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="experiment-1"></a>Experiment 1<a class="hash-link" href="#experiment-1" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="goal"></a>Goal<a class="hash-link" href="#goal" title="Direct link to heading">#</a></h3><p>Identify the improvement in performance over time for a single subject not present in the training dataset using active and semi-supervised learning methods.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="baseline"></a>Baseline<a class="hash-link" href="#baseline" title="Direct link to heading">#</a></h3><p>Obtain the baseline model for hand and pocket data using <code>Leave One Subject Out(LOSO)</code> and <code>hold out</code> validation separately. The classifier we decide to use is <strong>RandomForest</strong>.</p><ul><li>We train a model with hand data only and evaluate using LOSO. </li><li>In order to justify the poor performance of LOSO, we also evaluate using hold out validation(subject dependent). i.e. we train only on Subject 1 and test on Subject 1 data using a 70:30 split of the dataset. Similarly for all the other subjects as well.</li><li>We perform the same set of experiments using pocket data.</li><li>Each of the models are trained 5 times to account for <strong>variance in evaluation</strong>, we have plotted the error bars to show the variance in the accuracy score across <strong>5 different runs</strong> of the model.</li><li>The results are as shown below:
<img alt="img" src="/ASR/assets/images/baseline_1-4478691d1e89edae64bafb0f7ef4da3e.png"><img alt="img" src="/ASR/assets/images/baseline_2-76eb688a2b9cad5be69d2ecf4f9e919e.png"></li></ul><p>Unsurprisingly, the <strong>performance of hold-out is much better</strong> than that of LOSO as basically each individual activities are uncorrelated. It is hard to predict one subject&#x27;s activities correctly using other subject&#x27;s data.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="personalization-setup"></a>Personalization Setup<a class="hash-link" href="#personalization-setup" title="Direct link to heading">#</a></h3><p>To reiterate we have obtained our baseline using LOSO as discussed above, next we move on to active learning. In active learning, we expand our training data set using different query methods. That means we convert some unlabeled instances to labeled instances and add them to the training set by various criterions. After that, we retrain the model and test to get the updated accuracy results. It simulates continually learning from the activities of the individual subject over time. </p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</h5></div><div class="admonition-content"><p>Experimental parameters : we specify the number of iterations as 50, and the batch size for each iteration is 25. i.e In each iteration, we perform active learning with various criterions to select 25 instances from unlabeled data pool.</p></div></div><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="training-set"></a>Training Set<a class="hash-link" href="#training-set" title="Direct link to heading">#</a></h4><ul><li><em>Baseline</em>: Data from all of the subjects except subject <code>i</code> to build a baseline model, where the subject <code>i</code> is the test subject.</li><li><em>Personalization</em>: Subject <code>i</code> data split using a 70:30 split of the dataset. 70% split is taken as unlabeled data from subject <code>i</code> and subsets from this would be added to traning set at each round.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="test-set"></a>Test Set<a class="hash-link" href="#test-set" title="Direct link to heading">#</a></h4><ul><li>30% subset of the data from the subject <code>i</code>, which is the golden test data that never be added to the traning data set.</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="results"></a>Results<a class="hash-link" href="#results" title="Direct link to heading">#</a></h3><p>First, the total number of instances after feature extraction for each subject is listed below:</p><table><thead><tr><th>Category</th><th align="center">Subject</th><th align="center">Total number of instances</th><th align="center">Number of Golden Test instances</th><th align="center">Number of unlabeled instances in the pool</th></tr></thead><tbody><tr><td>hand</td><td align="center">1</td><td align="center">3769</td><td align="center">1243</td><td align="center">2526</td></tr><tr><td>hand</td><td align="center">2</td><td align="center">3311</td><td align="center">1092</td><td align="center">2219</td></tr><tr><td>hand</td><td align="center">3</td><td align="center">3637</td><td align="center">1200</td><td align="center">2437</td></tr><tr><td>hand</td><td align="center">4</td><td align="center">4015</td><td align="center">1324</td><td align="center">2691</td></tr><tr><td>hand</td><td align="center">5</td><td align="center">4376</td><td align="center">1444</td><td align="center">2932</td></tr><tr><td>hand</td><td align="center">6</td><td align="center">3983</td><td align="center">1314</td><td align="center">2669</td></tr><tr><td>hand</td><td align="center">7</td><td align="center">4007</td><td align="center">1322</td><td align="center">2685</td></tr><tr><td>pocket</td><td align="center">1</td><td align="center">2991</td><td align="center">987</td><td align="center">2004</td></tr><tr><td>pocket</td><td align="center">2</td><td align="center">2785</td><td align="center">919</td><td align="center">1866</td></tr><tr><td>pocket</td><td align="center">3</td><td align="center">2991</td><td align="center">987</td><td align="center">2004</td></tr><tr><td>pocket</td><td align="center">4</td><td align="center">3215</td><td align="center">1060</td><td align="center">2155</td></tr><tr><td>pocket</td><td align="center">5</td><td align="center">3486</td><td align="center">1150</td><td align="center">2336</td></tr><tr><td>pocket</td><td align="center">6</td><td align="center">3352</td><td align="center">1106</td><td align="center">2246</td></tr><tr><td>pocket</td><td align="center">7</td><td align="center">3590</td><td align="center">1184</td><td align="center">2406</td></tr></tbody></table><p>It is as expected that the <strong>accuracy increases</strong> as more labeled instances are added to the training set. In the comparison of different query methods, our experimental results show that the <code>active learning with least confidence</code> has the <strong>best performance</strong>, achieving <strong>70%-80%</strong> accuracy when <strong>20%</strong> of all original unlabeled data are added (500 out of 2500 samples) while the semi-supervised learning has the worst performance. </p><p>Furthermore, semi-supervised learning is <strong>not</strong> able to obtain <strong>enough new samples</strong> to retrain the model if we select only the <strong>samples with high confidence</strong>. Therefore, we use the <code>combination of active learning and semi-supervised learning</code> instead. But it still gets worse performance compared to other methods. The reason could be that the baseline classifier has a bad recognition rate, and it causes too many <strong>wrongly labeled data</strong> to be added to the training data set. Besides, one interesting result we obtain is that we can get a good result even if we just randomly query the unlabeled data during the initial iterations. This is because the model has not seen sufficient samples yet from the particular subject hence increase in the performance.</p><p><img alt="img" src="/ASR/assets/images/cmp_diff_method_1-68c3b3400b70f2d236a36beda9ca2f71.png">
<img alt="img" src="/ASR/assets/images/cmp_diff_method_2-1a69b97e136455e4dfd9f7b2e2be9a84.png">
<img alt="img" src="/ASR/assets/images/cmp_diff_method_3-76b7fcfb41687828f83b6f48c18d1045.png">
<img alt="img" src="/ASR/assets/images/cmp_diff_method_4-405e5577f7fd4b4d7298554369963ebf.png"></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="confusion-matrix-for-least-confidence-query-methodbest-performing"></a>Confusion matrix for least confidence query method(Best performing)<a class="hash-link" href="#confusion-matrix-for-least-confidence-query-methodbest-performing" title="Direct link to heading">#</a></h4><p><img alt="img" src="/ASR/assets/images/cnf_1-1f8d03f96f124540907e6af67ee0d742.png"></p><p>On the other hand, we also compared the performance of different sensor placements under a specific query method for each subject. The result shows that for each subject, we can get <strong>better performance with training and testing a model on pocket than on hand</strong>. This is reasonable since putting smartphone in pocket limits more to some extent the degree of freedom of the movement of a smartphone on pocket than on hand. Therefore, we can get a more similar pattern for different subjects on the pocket, and it becomes easier to personalize the model.</p><p><img alt="img" src="/ASR/assets/images/cmp_diff_sensor_placement_1-d645eb68ca1f5173771a816f4de4fd2a.png">
<img alt="img" src="/ASR/assets/images/cmp_diff_sensor_placement_2-950e5add538095f481290bc2f4be6762.png">
<img alt="img" src="/ASR/assets/images/cmp_diff_sensor_placement_3-7fa033934e3a8058183f905615e07778.png">
<img alt="img" src="/ASR/assets/images/cmp_diff_sensor_placement_4-3fad40d647cd5e56c01513e31a8db57f.png"></p><p>Furthermore, We also study the impact of the number of training subjects on the performance: As expected, the performance became worse with more training subjects used to build the original model. That means it is more difficult to adapt to the model when more other peopleâ€™s data is considered in the training model. </p><p><img alt="img" src="/ASR/assets/images/cmp_number_train_1-8574058965952ee273648cf6aa1399d2.png">
<img alt="img" src="/ASR/assets/images/cmp_number_train_2-78786d28aacdf8879dec627a504f4991.png">
<img alt="img" src="/ASR/assets/images/cmp_number_train_3-fe505d032cd5bd5c7a6293c8b8a2c983.png">
<img alt="img" src="/ASR/assets/images/cmp_number_train_4-ebcbf3cc4e5f4581b663417ec4602c10.png"></p><p>In order to understand this claim better let&#x27;s look at a specific example below, for training with <strong>1-3 subjects</strong> we can see that the <strong>baseline performance is low</strong> on the right plot and with more training data the baseline performance is higher. Whereas when we look at the performance of the model after adding the <strong>1200 samples</strong> using active learning the one with less training data(<em>blue,green and red dotted line</em>) on the left plot is able to personalize better. This is an interesting observation and indicates a tradeoff between baseline performance vs personalization.</p><p><img alt="img" src="/ASR/assets/images/cmp_tradeoff-0c01ca689c05cfd1795cbe61563f07b0.png"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="experiment-2"></a>Experiment 2<a class="hash-link" href="#experiment-2" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="goal-1"></a>Goal<a class="hash-link" href="#goal-1" title="Direct link to heading">#</a></h3><p>Identify how can a classifier be adapted to account for sensor placement variations for a single individual subject</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="baseline-1"></a>Baseline<a class="hash-link" href="#baseline-1" title="Direct link to heading">#</a></h3><p>We obtain baseline model for training on hand data of each subject separately and testing on pocket data of that corresponding subject with a RandomForest classifer. This becomes the baseline for Experiment 2 where we try to build a robust classifier to account for sensor placement variations for individual subjects. We can see that the performance of the classifier is very low as shown in the plot, which is expected because when we train a model to recognize activity by recording data with mobile phone on hand from a single subject and then test it on data recorded with mobile phone in pocket, the accuracy is bound to be low.</p><p><img alt="img" src="/ASR/assets/images/baseline_3-30592632f3be2bf3feeb8c3797e5de37.png"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="adaptation-setup"></a>Adaptation Setup<a class="hash-link" href="#adaptation-setup" title="Direct link to heading">#</a></h3><p>We keep expanding the training set and retrain the model by adding more pocket data using active learning and semi-supervised learning, and observe the variation on both pocket/hand test data sets.</p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</h5></div><div class="admonition-content"><p>Experimental parameters : we specify the number of iterations as 50, and the batch size for each iteration is 25. i.e In each iteration, we perform active learning with various criterions to select 25 instances from unlabeled data pool.</p></div></div><p>Subject <code>i</code> hand and pocket data split using a 70:30 split of the dataset</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="training-set-1"></a>Training Set<a class="hash-link" href="#training-set-1" title="Direct link to heading">#</a></h4><ul><li><em>Baseline</em>: 70% Hand data of a specific subject <code>i</code> </li><li><em>Adaptation</em>: subsets from 70% pocket data of a specific subject <code>i</code></li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="test-set-1"></a>Test Set<a class="hash-link" href="#test-set-1" title="Direct link to heading">#</a></h4><ul><li>30% subsets of hand data and pocket data of a specific subject <code>i</code></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="results-1"></a>Results<a class="hash-link" href="#results-1" title="Direct link to heading">#</a></h3><p>We can observe that the performance of placement on hand remains stably high as more unlabeled pocket data is converted to labeled data and added to the training set. On the other hand, we can also observe that the performance of a personalized classifier would increase exponentially at first, and then reach an upper bound close to the other placement when a certain number of training data are newly added. It indicates that we can adapt a classifier with active learning to get a good performance of activity recognition even if the placement changes.</p><p><img alt="img" src="/ASR/assets/images/train_test_diff_placement_3-df3c3a465e90171182f926a7b6edd988.png">
<img alt="img" src="/ASR/assets/images/train_test_diff_placement_4-f7c506b921d38eff89f159e00fb6d5c7.png">
<img alt="img" src="/ASR/assets/images/train_test_diff_placement_1-31a200be41bb2146b341b372cce04f72.png">
<img alt="img" src="/ASR/assets/images/train_test_diff_placement_2-08827941dc1488e662eb2e9f97ca12c8.png"></p></div></article><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/ASR/docs/doc7"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Methods</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/ASR/docs/doc9"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Conclusion Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_ thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#experiment-1" class="table-of-contents__link">Experiment 1</a><ul><li><a href="#goal" class="table-of-contents__link">Goal</a></li><li><a href="#baseline" class="table-of-contents__link">Baseline</a></li><li><a href="#personalization-setup" class="table-of-contents__link">Personalization Setup</a></li><li><a href="#results" class="table-of-contents__link">Results</a></li></ul></li><li><a href="#experiment-2" class="table-of-contents__link">Experiment 2</a><ul><li><a href="#goal-1" class="table-of-contents__link">Goal</a></li><li><a href="#baseline-1" class="table-of-contents__link">Baseline</a></li><li><a href="#adaptation-setup" class="table-of-contents__link">Adaptation Setup</a></li><li><a href="#results-1" class="table-of-contents__link">Results</a></li></ul></li></ul></div></div></div></div></main></div></div></div>
<script src="/ASR/styles.71d10a56.js"></script>
<script src="/ASR/runtime~main.366c8f14.js"></script>
<script src="/ASR/main.6fc68446.js"></script>
<script src="/ASR/1.8f67cab4.js"></script>
<script src="/ASR/35.da5c47bd.js"></script>
<script src="/ASR/36.a067ae9c.js"></script>
<script src="/ASR/f976f453.247ede68.js"></script>
<script src="/ASR/34.1489086d.js"></script>
<script src="/ASR/b849be5b.bd3ae262.js"></script>
</body>
</html>