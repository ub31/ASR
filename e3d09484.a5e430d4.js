(window.webpackJsonp=window.webpackJsonp||[]).push([[30],{103:function(e,t,a){"use strict";a.d(t,"a",(function(){return b})),a.d(t,"b",(function(){return u}));var n=a(0),r=a.n(n);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function s(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?s(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function c(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=r.a.createContext({}),d=function(e){var t=r.a.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},b=function(e){var t=d(e.components);return r.a.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},p=r.a.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,s=e.parentName,l=c(e,["components","mdxType","originalType","parentName"]),b=d(a),p=n,u=b["".concat(s,".").concat(p)]||b[p]||m[p]||i;return a?r.a.createElement(u,o(o({ref:t},l),{},{components:a})):r.a.createElement(u,o({ref:t},l))}));function u(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,s=new Array(i);s[0]=p;var o={};for(var c in t)hasOwnProperty.call(t,c)&&(o[c]=t[c]);o.originalType=e,o.mdxType="string"==typeof e?e:n,s[1]=o;for(var l=2;l<i;l++)s[l]=a[l];return r.a.createElement.apply(null,s)}return r.a.createElement.apply(null,a)}p.displayName="MDXCreateElement"},248:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/71-c034c688c5e7c7913aedb1667a1f06a7.png"},249:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/AL-559ffa17fd9aae2de5175750035996a2.png"},250:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/algo2-e37329f74c6d7fe928a8d3a71da79f05.png"},251:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/algo1-54de6a09386250056c225ebc002bfeac.png"},252:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/algo3-47258f05cad10af31e047439de7c19f1.png"},253:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/algo4-525ece7441ef4e4c579c33800eec694c.png"},254:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/semi-supervised-9e9b79fb1cda14493489f196fe397984.png"},97:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return s})),a.d(t,"metadata",(function(){return o})),a.d(t,"rightToc",(function(){return c})),a.d(t,"default",(function(){return d}));var n=a(3),r=a(7),i=(a(0),a(103)),s={id:"doc7",title:"Methods",sidebar_label:"Methods",custom_edit_url:null},o={unversionedId:"doc7",id:"doc7",isDocsHomePage:!1,title:"Methods",description:"As we know, the pipeline for activity recognition is as shown below, and we have discussed the dataset collection in the previous section.",source:"@site/docs/doc7.md",slug:"/doc7",permalink:"/ASR/docs/doc7",editUrl:null,version:"current",sidebar_label:"Methods",sidebar:"someSidebar",previous:{title:"Activity Exploration",permalink:"/ASR/docs/doc6"},next:{title:"Experimental Result & Analysis",permalink:"/ASR/docs/doc8"}},c=[{value:"Feature Extraction",id:"feature-extraction",children:[]},{value:"Active learning",id:"active-learning",children:[{value:"Ensembling with entropy",id:"ensembling-with-entropy",children:[]},{value:"Least confidence",id:"least-confidence",children:[]},{value:"Minimum margin",id:"minimum-margin",children:[]},{value:"Random sampling",id:"random-sampling",children:[]}]},{value:"Semi-supervised learning",id:"semi-supervised-learning",children:[]}],l={rightToc:c};function d(e){var t=e.components,s=Object(r.a)(e,["components"]);return Object(i.b)("wrapper",Object(n.a)({},l,s,{components:t,mdxType:"MDXLayout"}),Object(i.b)("p",null,"As we know, the pipeline for activity recognition is as shown below, and we have discussed the dataset collection in the previous section."),Object(i.b)("p",null,Object(i.b)("img",{alt:"img",src:a(248).default})),Object(i.b)("h3",{id:"feature-extraction"},"Feature Extraction"),Object(i.b)("p",null,"We move on to the next step which is ",Object(i.b)("strong",{parentName:"p"},"frame")," and ",Object(i.b)("strong",{parentName:"p"},"feature extraction"),".  We perform basic preprocessing to drop samples in the start and end of the data for each activity for all subjects. The frames are extracted for a window of ",Object(i.b)("strong",{parentName:"p"},"100ms")," with ",Object(i.b)("strong",{parentName:"p"},"50ms overlapping")," frames. Since our goal is to study personalization, we did not experiment on different frame sizes. The data is recorded at ",Object(i.b)("strong",{parentName:"p"},"100Hz")," hence we selected 100ms frames with 50% overlap. We have used time-domain features that have worked well in previous work designed for accelerometer data. We believe that since the accelerometer signals are captured in time domain, these features are sufficient to model the activities. "),Object(i.b)("p",null,"We extracted the following features for each ",Object(i.b)("inlineCode",{parentName:"p"},"frame, mean, variance, standard deviation, median, third and fourth moment, 25,50 and 75 percentile, skewness, kurtosis and rms of signal"),". We obtain the above features for the raw accelerometer signals along X,Y and Z axis as well as the magnitude signal. In total we get 48 features. The choice of these features are based upon previous works as well as assignments in class. "),Object(i.b)("table",null,Object(i.b)("thead",{parentName:"table"},Object(i.b)("tr",{parentName:"thead"},Object(i.b)("th",Object(n.a)({parentName:"tr"},{align:null}),"Features Extracted"),Object(i.b)("th",Object(n.a)({parentName:"tr"},{align:"center"}),"Number of Features"))),Object(i.b)("tbody",{parentName:"table"},Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Mean"),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),"4 (AccX,AccY,AccZ,MagAcc)")),Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Median"),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),"4")),Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Variance"),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),"4")),Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Standard Deviation"),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),"4")),Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Third Moment"),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),"4")),Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Fourth Moment"),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),"4")),Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"25th Percentile"),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),"4")),Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"50th Percentile"),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),"4")),Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"75th Percentile"),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),"4")),Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Skewness"),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),"4")),Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Kurtosis"),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),"4")),Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"RMS"),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),"4")),Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:null}),Object(i.b)("strong",{parentName:"td"},"Total")),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),"48")))),Object(i.b)("p",null,"Once we obtain the features, we train our baseline models, this is discussed in detail in the next section. We use a ",Object(i.b)("strong",{parentName:"p"},"RandomForestClassifier")," as the machine learning model. The choice of RandomForestClassifier is from prior literature review as well as widespread use of ensembling methods. Ensemble methods help to achieve the ",Object(i.b)("inlineCode",{parentName:"p"},"bias-variance tradeoff"),". When the ",Object(i.b)("strong",{parentName:"p"},"number of estimators")," ",Object(i.b)("em",{parentName:"p"},"increases")," the ",Object(i.b)("strong",{parentName:"p"},"variance")," can be reduced, by averaging out across the different base learners(decision tree). In addition to this random forest identifies ",Object(i.b)("strong",{parentName:"p"},"different subsets of features")," to train the different base learners thereby ",Object(i.b)("strong",{parentName:"p"},"preventing overfitting"),"."),Object(i.b)("h2",{id:"active-learning"},"Active learning"),Object(i.b)("p",null,"We identify the most informative samples or the samples where the model is uncertain about it\u2019s prediction and use a pool-based strategy to query for these samples. The setup for this is, there is a set of labeled instances, a set of unlabeled instances, and a golden oracle which provides the label for an unlabeled instance when queried. During each iteration we find the instance which we want the oracle to label using different query strategies and then add them to the labeled pool to retrain the model and evaluate them. In this way the model starts to gradually learn from the additional samples provided."),Object(i.b)("p",null,Object(i.b)("img",{alt:"img",src:a(249).default})),Object(i.b)("p",null,"The main query strategies which we analyzed are based on Uncertainty sampling. The usefulness of a instance is calculated based on different measures like classification uncertainty, classification margin and classification entropy. We briefly explain the measures below, "),Object(i.b)("h3",{id:"ensembling-with-entropy"},"Ensembling with entropy"),Object(i.b)("p",null,"In this method, we use two models (RandomForest and MLPClassifier) to train a classifier and then evaluate the predictions of both the classifiers on the unlabeled instance.  Then we measure the ",Object(i.b)("strong",{parentName:"p"},"entropy in the probability distribution of the predictions")," from the two models. This shows if the unlabeled instance has a different probability distribution from one model to another which indicates uncertainty."),Object(i.b)("p",null,Object(i.b)("img",{alt:"img",src:a(250).default})),Object(i.b)("h3",{id:"least-confidence"},"Least confidence"),Object(i.b)("p",null,"This is one of the simplest methods to measure the uncertainty. We do this by measuring the prediction probability from the model, and select the samples whose ",Object(i.b)("strong",{parentName:"p"},"confident predictions are less than 0.5")," as uncertain. This implies that the model is able to predict the class only with 0.5 probability making it uncertain."),Object(i.b)("p",null,Object(i.b)("img",{alt:"img",src:a(251).default})),Object(i.b)("h3",{id:"minimum-margin"},"Minimum margin"),Object(i.b)("p",null,"The most intuitive form of uncertainty sampling is the difference between the two most confident predictions. We measure this by identifying for the label that the model predicted, how confident is this prediction than the next most confident label prediction by the model. In other words it measures the ",Object(i.b)("strong",{parentName:"p"},"difference in confidence between the first and second most likely predictions"),". "),Object(i.b)("p",null,Object(i.b)("img",{alt:"img",src:a(252).default})),Object(i.b)("h3",{id:"random-sampling"},"Random sampling"),Object(i.b)("p",null,"For random sampling, we add ",Object(i.b)("strong",{parentName:"p"},"randomly selected samples")," from the unlabeled pool without any criterion and to the labeled set."),Object(i.b)("p",null,Object(i.b)("img",{alt:"img",src:a(253).default})),Object(i.b)("h2",{id:"semi-supervised-learning"},"Semi-supervised learning"),Object(i.b)("p",null,"The major difference between active learning and semi-supervised learning is that the latter uses the unlabeled examples for improving the accuracy whereas active learning approaches query an oracle to label the data."),Object(i.b)("p",null,"The goal of the semi-supervised approach is to understand whether we can label the unlabeled samples from the model and use it to retrain the model. For this method, we have a set of labeled instances, a set of unlabeled instances, and a test set."),Object(i.b)("p",null,"We implement self-training which makes use of a model\u2019s own prediction on unlabeled data in order to obtain additional information that can be used during training. The basic idea is to build a model on the labeled data and then use this model to estimate the labels for the unlabeled pool. The most confident label values are taken and the newly labeled data are then used along with the originally labeled instances to retrain the model. This procedure is repeated until all samples are labeled."),Object(i.b)("p",null,Object(i.b)("img",{alt:"img",src:a(254).default})),Object(i.b)("p",null,"However, semi-supervised learning is not able to obtain enough new samples to retrain the model if we select only the samples with high confidence. We implement a mixed strategy consisting of active and semi- supervised, where at each iteration we sample uncertain samples and query the oracle for the labels to add to the training data and then use another set of instances, identify their predictions in a self-training manner and then add the most confident predictions to the training dataset with the predicted labels as the actual labels."))}d.isMDXComponent=!0}}]);